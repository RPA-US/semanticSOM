[project]
name = "bpmextension"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = "==3.10.*"
dependencies = [
    "polars==1.20.0",
    "python-statemachine==2.5.0",
    "llama-cpp-python==0.3.*",
    "torch>=2.5.1",
    "torchvision>=0.20.1",
    "accelerate>=1.3.0",
    "auto-gptq>=0.7.1",
    "jax>=0.5.0",
    "langchain>=0.3.16",
    "langchain-community>=0.3.16",
    "qwen-vl-utils>=0.0.10",
    "optimum>=1.23.3",
    "transformers>=4.48.1",
    "sqlite-vec>=0.1.6",
    "imagehash>=4.3.2",
    "opencv-python>=4.11.0.86",
    "shapely>=2.0.7",
    "openai>=1.63.2",
    "spacy>=3.8.4",
    "sentence-transformers>=3.4.1",
    "uv>=0.6.6",
    "einops>=0.8.1",
    "timm>=1.0.15",
    "autoawq",
    "bitsandbytes>=0.45.3",
]

[dependency-groups]
dev = [
    "setuptools>=70.0.0",
    "pre-commit==4.0.1",
    "coverage>=7.6.12",
]

[build-system]
requires = ["setuptools"]
build-backend = "setuptools.build_meta"

[tool.setuptools.packages]
find = {}

[[tool.uv.index]]
name = "torch-gpu"
url = "https://download.pytorch.org/whl/cu124"

[[tool.uv.index]]
name = "llama-cpp-python-gpu"
url = "https://abetlen.github.io/llama-cpp-python/whl/cu124"

[tool.uv.sources]
autoawq = { git = "https://github.com/casper-hansen/AutoAWQ.git" }

[project.scripts]
dev-setup = "scripts.dev_setup:main"
eval = "src.mllm_judge.api_benchmarks:main"

[project.optional-dependencies]
eval = [
    "dashscope>=1.22.1",
    "google-genai>=1.2.0",
    "google-generativeai>=0.8.4",
    "latex>=0.7.0",
    "matplotlib[latex]>=3.10.1",
    "replicate>=1.0.4",
    "seaborn>=0.13.2",
    "statsmodels>=0.14.4",
]

[tool.ruff]

fix = true
show-fixes = true
include = ["pyproject.toml", "src/**/*.py", "src/**/*.ipynb", "scripts/**/*.py"]
exclude = ["src/semantics/prompts.py"]


[tool.ruff.lint]

ignore = ["F841", "E203", "E501"]

[tool.ruff.format]

docstring-code-format = true

[tool.mypy]

ignore_missing_imports = true
